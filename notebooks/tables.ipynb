{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Tables Generator\n",
    "Reads `full_HDBSCAN_metadata.csv` (train/test) and `best_per_model_HDBSCAN.csv` to produce LaTeX tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "TRAIN_FULL  = \"../compare_flu/results/full_HDBSCAN_metadata.csv\"\n",
    "TEST_FULL   = \"../compare_flu2018-2020/results/full_HDBSCAN_metadata.csv\"\n",
    "BEST_MODEL  = \"../compare_flu2018-2020/results/best_per_model_HDBSCAN.csv\"\n",
    "\n",
    "train = pd.read_csv(TRAIN_FULL)\n",
    "test  = pd.read_csv(TEST_FULL)\n",
    "best  = pd.read_csv(BEST_MODEL)\n",
    "\n",
    "print(\"train:\", train.shape, \"  test:\", test.shape, \"  best:\", best.shape)\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── helpers ──────────────────────────────────────────────────────────────────\n",
    "\n",
    "MODEL_DISPLAY = {\n",
    "    \"t33-650M\": \"ESM-2(650M)\",\n",
    "    \"t36-3B\":   \"ESM-2(3B)\",\n",
    "    \"t48-15B\":  \"ESM-2(15B)\",\n",
    "    \"protbert\": \"ProtBert\",\n",
    "    \"prot5\":    \"ProtT5\",\n",
    "    \"CARP\":     \"CARP\",\n",
    "}\n",
    "POOL_DISPLAY = {\n",
    "    \"mean\":        \"mean\",\n",
    "    \"bos\":         \"BOS\",\n",
    "    \"attentionmean\": \"attn-mean\",\n",
    "    \"sitemean\":    \"site-mean\",\n",
    "}\n",
    "DIM_DISPLAY = {\n",
    "    \"t-sne\": \"t-SNE\",\n",
    "    \"umap\":  \"UMAP\",\n",
    "    \"pca\":   \"PCA\",\n",
    "    \"mds\":   \"MDS\",\n",
    "}\n",
    "\n",
    "def extract_epsilon(predicted_clusters_column):\n",
    "    \"\"\"Pull the distance threshold out of e.g. 'method_cluster_at_1.5'.\"\"\"\n",
    "    m = re.search(r'_cluster_at_([\\d.]+)$', str(predicted_clusters_column))\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def parse_method(method):\n",
    "    \"\"\"Return dict with dim, metric, components, base_model, pooling, gene.\"\"\"\n",
    "    if not method.startswith(\"reduced_\"):\n",
    "        return None\n",
    "    parts = method.split(\"_\")\n",
    "    # parts: ['reduced', dim, metric, components, *model_parts, [gene]]\n",
    "    dim        = parts[1]          # e.g. 't-sne'\n",
    "    metric     = parts[2]          # e.g. 'cosine'\n",
    "    components = int(parts[3])\n",
    "    rest       = parts[4:]         # e.g. ['t33-650M-mean', 'All'] or ['prot5']\n",
    "    if rest and rest[-1] == \"All\":\n",
    "        gene       = \"All\"\n",
    "        model_full = \"_\".join(rest[:-1])  # e.g. 't33-650M-mean'\n",
    "    else:\n",
    "        gene       = None\n",
    "        model_full = \"_\".join(rest)       # e.g. 'prot5'\n",
    "    # split model_full into base model and pooling (last hyphen-separated token)\n",
    "    tokens = model_full.split(\"-\")\n",
    "    # pooling is the last token if it's a known pooling key\n",
    "    if tokens[-1] in POOL_DISPLAY:\n",
    "        pooling    = tokens[-1]\n",
    "        base_model = \"-\".join(tokens[:-1])\n",
    "    else:\n",
    "        pooling    = None\n",
    "        base_model = model_full\n",
    "    return dict(dim=dim, metric=metric, components=components,\n",
    "                base_model=base_model, pooling=pooling, gene=gene)\n",
    "\n",
    "def model_label(base_model):\n",
    "    return MODEL_DISPLAY.get(base_model, base_model)\n",
    "\n",
    "train[\"epsilon\"] = train[\"predicted_clusters_column\"].apply(extract_epsilon)\n",
    "test[\"epsilon\"]  = test[\"predicted_clusters_column\"].apply(extract_epsilon)\n",
    "print(\"epsilon sample:\", train[\"epsilon\"].dropna().head(3).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1 – Baseline scores (genetic dim-reduction methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_METHODS = [\"t-sne\", \"umap\", \"pca\", \"mds\"]\n",
    "\n",
    "# best train row per baseline method (lowest NVI)\n",
    "train_base = (\n",
    "    train[train[\"method\"].isin(BASELINE_METHODS)]\n",
    "    .sort_values(\"normalized_vi\")\n",
    "    .groupby(\"method\", sort=False)\n",
    "    .first()\n",
    "    .reset_index()[[\"method\", \"epsilon\", \"normalized_vi\",\n",
    "                    \"adjusted_rand_score\", \"normalized_mutual_info_score\"]]\n",
    "    .rename(columns={\n",
    "        \"normalized_vi\":              \"train_nvi\",\n",
    "        \"adjusted_rand_score\":        \"train_ari\",\n",
    "        \"normalized_mutual_info_score\": \"train_nmi\",\n",
    "    })\n",
    ")\n",
    "\n",
    "# matching test row (single optimal row per method in test set)\n",
    "test_base = (\n",
    "    test[test[\"method\"].isin(BASELINE_METHODS)]\n",
    "    [[\"method\", \"normalized_vi\", \"adjusted_rand_score\", \"normalized_mutual_info_score\"]]\n",
    "    .rename(columns={\n",
    "        \"normalized_vi\":              \"test_nvi\",\n",
    "        \"adjusted_rand_score\":        \"test_ari\",\n",
    "        \"normalized_mutual_info_score\": \"test_nmi\",\n",
    "    })\n",
    ")\n",
    "\n",
    "baseline = train_base.merge(test_base, on=\"method\")\n",
    "# enforce display order\n",
    "order = {m: i for i, m in enumerate(BASELINE_METHODS)}\n",
    "baseline[\"_ord\"] = baseline[\"method\"].map(order)\n",
    "baseline = baseline.sort_values(\"_ord\").drop(columns=\"_ord\")\n",
    "baseline[\"method\"] = baseline[\"method\"].map(lambda m: DIM_DISPLAY.get(m, m))\n",
    "\n",
    "baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_baseline(df):\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        rows.append(\n",
    "            f\"{r['method']:<8} & {r['epsilon']:<5} \"\n",
    "            f\"& {r['train_nvi']:.4f} & {r['test_nvi']:.4f} \"\n",
    "            f\"& {r['train_nmi']:.4f} & {r['test_nmi']:.4f} \"\n",
    "            f\"& {r['train_ari']:.4f} & {r['test_ari']:.4f} \\\\\\\\\"\n",
    "        )\n",
    "    header = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Baseline scores. Lower NVI is better, higher NMI and ARI are better.}\n",
    "\\label{tab:baseline-results}\n",
    "\\begin{tabular}{|l|l|l|l|l|l|l|l|}\n",
    "\\hline\n",
    "Method & HDBSCAN~$\\epsilon$ & Train NVI & Test NVI & Train NMI & Test NMI & Train ARI & Test ARI\\\\\\\\\n",
    "\\hline\"\"\"\n",
    "    footer = r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    print(header)\n",
    "    print(\"\\n\".join(rows))\n",
    "    print(footer)\n",
    "\n",
    "latex_baseline(baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2 – Top 10 training PLM results (mean pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Only reduced-PLM methods\n# For ESM-2 models: keep mean pooling only; CARP/ProtBert/ProtT5 have no pooling suffix so keep all\nplm_mask = train[\"method\"].str.startswith(\"reduced_\") & ~train[\"method\"].isin(BASELINE_METHODS)\nplm_train = train[plm_mask].copy()\n\nparsed = plm_train[\"method\"].apply(parse_method)\nplm_train = plm_train.join(pd.DataFrame(parsed.tolist(), index=plm_train.index))\nplm_train = plm_train[plm_train[\"pooling\"].isin([\"mean\"]) | plm_train[\"pooling\"].isna()]\n\ntop10 = (\n    plm_train.sort_values(\"normalized_vi\")\n    .head(10)\n    [[\"base_model\", \"dim\", \"metric\", \"components\", \"epsilon\",\n      \"normalized_vi\", \"adjusted_rand_score\", \"normalized_mutual_info_score\"]]\n    .copy()\n)\ntop10[\"base_model\"] = top10[\"base_model\"].map(model_label)\ntop10[\"dim\"]        = top10[\"dim\"].map(lambda d: DIM_DISPLAY.get(d, d))\ntop10[\"metric\"]     = top10[\"metric\"].str.capitalize()\ntop10\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_top10(df):\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        rows.append(\n",
    "            f\"{r['base_model']:<14} & {r['dim']:<6} & {r['metric']:<10} \"\n",
    "            f\"& {int(r['components'])} & {r['epsilon']:<5} \"\n",
    "            f\"& {r['normalized_vi']:.4f} \"\n",
    "            f\"& {r['normalized_mutual_info_score']:.4f} \"\n",
    "            f\"& {r['adjusted_rand_score']:.4f} \\\\\\\\\"\n",
    "        )\n",
    "    header = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Top 10 training results using protein language model embeddings and mean-pooling.\n",
    "Lower NVI is better, higher NMI and ARI are better.}\n",
    "\\label{tab:top10-results}\n",
    "\\begin{tabular}{|l|l|l|c|c|c|c|c|}\n",
    "\\hline\n",
    "Model & Dim.~red. & Metric & Dims & HDBSCAN~$\\epsilon$ & NVI & NMI & ARI\\\\\\\\\n",
    "\\hline\"\"\"\n",
    "    footer = r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    print(header)\n",
    "    print(\"\\n\".join(rows))\n",
    "    print(footer)\n",
    "\n",
    "latex_top10(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3 – Best configuration per model (train + test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# best_per_model already has train/test side by side\n# need epsilon from train full metadata (optimal row)\ntrain_opt = (\n    train.sort_values(\"normalized_vi\")\n    .groupby(\"method\", sort=False)\n    .first()\n    .reset_index()[[\"method\", \"epsilon\"]]\n)\n\nbest_with_eps = best.merge(train_opt, on=\"method\", how=\"left\")\n\n# parse method for display\nparsed_best = best_with_eps[\"method\"].apply(parse_method)\nbest_with_eps = best_with_eps.join(pd.DataFrame(parsed_best.tolist(), index=best_with_eps.index))\n\n# for baseline methods (no 'reduced_' prefix), fill display columns manually\ndef fill_display(row):\n    if pd.isna(row.get(\"dim\")):\n        return DIM_DISPLAY.get(row[\"method\"], row[\"method\"])\n    return DIM_DISPLAY.get(row[\"dim\"], row[\"dim\"])\n\nbest_with_eps[\"dim_label\"]    = best_with_eps.apply(fill_display, axis=1)\nbest_with_eps[\"model_label\"]  = best_with_eps[\"model\"].map(lambda m: MODEL_DISPLAY.get(m, m))\nbest_with_eps[\"metric_label\"] = best_with_eps[\"metric\"].fillna(\"\").str.capitalize()\n\ncols = [\"model_label\", \"dim_label\", \"metric_label\", \"components\", \"epsilon\",\n        \"train_normalized_vi\", \"test_normalized_vi\",\n        \"train_normalized_mutual_info_score\", \"test_normalized_mutual_info_score\",\n        \"train_adjusted_rand_score\", \"test_adjusted_rand_score\"]\nbest_with_eps[cols]\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_best_per_model(df):\n",
    "    rows = []\n",
    "    for _, r in df.iterrows():\n",
    "        comp = int(r['components']) if pd.notna(r.get('components')) else \"--\"\n",
    "        met  = r['metric_label'] if r['metric_label'] else \"--\"\n",
    "        rows.append(\n",
    "            f\"{r['model_label']:<14} & {r['dim_label']:<7} & {met:<10} \"\n",
    "            f\"& {comp} & {r['epsilon']:<5} \"\n",
    "            f\"& {r['train_normalized_vi']:.4f} & {r['test_normalized_vi']:.4f} \"\n",
    "            f\"& {r['train_normalized_mutual_info_score']:.4f} & {r['test_normalized_mutual_info_score']:.4f} \"\n",
    "            f\"& {r['train_adjusted_rand_score']:.4f} & {r['test_adjusted_rand_score']:.4f} \\\\\\\\\"\n",
    "        )\n",
    "    header = r\"\"\"\n",
    "\\begin{table}[h]\n",
    "\\centering\n",
    "\\caption{Best configuration for each model type.\n",
    "Lower NVI is better, higher NMI and ARI are better.}\n",
    "\\label{tab:best-config}\n",
    "\\begin{tabular}{|l|l|l|c|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "Model & Dim.~red. & Metric & Dims & $\\epsilon$ & Train NVI & Test NVI & Train NMI & Test NMI & Train ARI & Test ARI\\\\\\\\\n",
    "\\hline\"\"\"\n",
    "    footer = r\"\"\"\n",
    "\\hline\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "    print(header)\n",
    "    print(\"\\n\".join(rows))\n",
    "    print(footer)\n",
    "\n",
    "latex_best_per_model(best_with_eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 4 – Pooling method comparison (test NVI/NMI/ARI per model × pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use best_per_model rows that have a pooling type (i.e. ESM-2 / ProtBert / ProtT5 / CARP)\n",
    "pool_rows = best_with_eps[best_with_eps[\"pooling\"].notna()].copy()\n",
    "\n",
    "pool_rows[\"pooling_label\"] = pool_rows[\"pooling\"].map(POOL_DISPLAY)\n",
    "\n",
    "pivot_nvi = pool_rows.pivot_table(\n",
    "    index=\"model_label\", columns=\"pooling_label\",\n",
    "    values=\"test_normalized_vi\", aggfunc=\"first\"\n",
    ")\n",
    "pivot_nmi = pool_rows.pivot_table(\n",
    "    index=\"model_label\", columns=\"pooling_label\",\n",
    "    values=\"test_normalized_mutual_info_score\", aggfunc=\"first\"\n",
    ")\n",
    "pivot_ari = pool_rows.pivot_table(\n",
    "    index=\"model_label\", columns=\"pooling_label\",\n",
    "    values=\"test_adjusted_rand_score\", aggfunc=\"first\"\n",
    ")\n",
    "\n",
    "print(\"NVI by pooling:\")\n",
    "display(pivot_nvi.round(4))\n",
    "print(\"\\nNMI by pooling:\")\n",
    "display(pivot_nmi.round(4))\n",
    "print(\"\\nARI by pooling:\")\n",
    "display(pivot_ari.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latex_pooling_table(pool_rows, metric_col, caption, label):\n",
    "    POOL_ORDER = [\"mean\", \"BOS\", \"attn-mean\", \"site-mean\"]\n",
    "    present = [p for p in POOL_ORDER if p in pool_rows[\"pooling_label\"].values]\n",
    "\n",
    "    header = (\n",
    "        f\"\\\\begin{{table}}[h]\\n\\\\centering\\n\"\n",
    "        f\"\\\\caption{{{caption}}}\\n\\\\label{{{label}}}\\n\"\n",
    "        f\"\\\\begin{{tabular}}{{|l|{'c|' * len(present)}}}\\n\\\\hline\\n\"\n",
    "        f\"Model & {' & '.join(present)} \\\\\\\\\\n\\\\hline\"\n",
    "    )\n",
    "    rows = []\n",
    "    for model, grp in pool_rows.groupby(\"model_label\"):\n",
    "        vals = {r[\"pooling_label\"]: r[metric_col] for _, r in grp.iterrows()}\n",
    "        cells = \" & \".join(f\"{vals.get(p, float('nan')):.4f}\" if p in vals else \"--\" for p in present)\n",
    "        rows.append(f\"{model} & {cells} \\\\\\\\\")\n",
    "    footer = \"\\\\hline\\n\\\\end{tabular}\\n\\\\end{table}\"\n",
    "    print(header)\n",
    "    print(\"\\n\".join(rows))\n",
    "    print(footer)\n",
    "\n",
    "latex_pooling_table(\n",
    "    pool_rows, \"test_normalized_vi\",\n",
    "    \"Test NVI by model and pooling strategy. Lower is better.\",\n",
    "    \"tab:pooling-nvi\"\n",
    ")\n",
    "print()\n",
    "latex_pooling_table(\n",
    "    pool_rows, \"test_normalized_mutual_info_score\",\n",
    "    \"Test NMI by model and pooling strategy. Higher is better.\",\n",
    "    \"tab:pooling-nmi\"\n",
    ")\n",
    "print()\n",
    "latex_pooling_table(\n",
    "    pool_rows, \"test_adjusted_rand_score\",\n",
    "    \"Test ARI by model and pooling strategy. Higher is better.\",\n",
    "    \"tab:pooling-ari\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}